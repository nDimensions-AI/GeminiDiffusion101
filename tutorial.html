<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Evolution of AI Architectures: From CNNs to Diffusion</title>
    <!-- Chosen Palette: Celestial Purple/Blue Gradient -->
    <!-- Application Structure Plan: A slide-based presentation, inspired by the user's provided framework. Each of the 18 validated story points is a separate, full-screen slide. This format creates a focused, step-by-step tutorial. Navigation is handled by 'Previous' and 'Next' buttons and keyboard arrows, providing a controlled and guided user experience. This structure is excellent for delivering a dense, chronological narrative without overwhelming the user. -->
    <!-- Visualization & Content Choices: GOAL: To create a visually rich, engaging slideshow tutorial. METHOD: Each slide uses large, bold typography, colored "problem" and "solution" boxes, and comparison cards to structure information clearly. Images are sourced directly from the user's provided presentation where applicable (for RNNs, LSTMs, etc.) and supplemented with conceptual placeholders for newer topics. The design uses gradients and animations to feel dynamic and modern. INTERACTION: The primary interaction is navigating between slides. This simple, focused interaction model keeps the user engaged with the content flow. -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .presentation-container {
            width: 100%;
            min-height: 100vh;
            position: relative;
        }
        .slide {
            width: 100%;
            min-height: 100vh;
            position: absolute;
            top: 0;
            left: 0;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 80px 20px 120px 20px;
            text-align: center;
            opacity: 0;
            transform: scale(0.95);
            transition: opacity 0.6s ease-in-out, transform 0.6s ease-in-out;
            background: rgba(255, 255, 255, 0.95);
            overflow-y: auto;
            visibility: hidden;
        }
        .slide.active {
            opacity: 1;
            transform: scale(1);
            z-index: 10;
            visibility: visible;
        }
        .slide h1 {
            font-size: 3rem;
            color: #2c3e50;
            margin-bottom: 25px;
            font-weight: 700;
        }
        .slide h2 {
            font-size: 2.2rem;
            color: #34495e;
            margin-bottom: 20px;
            font-weight: 600;
        }
        .slide p {
            font-size: 1.2rem;
            line-height: 1.7;
            color: #2c3e50;
            max-width: 900px;
            margin-bottom: 15px;
        }
        .content-wrapper {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            width: 100%;
            max-width: 1100px;
        }
        .text-content {
            flex: 1;
            max-width: 800px;
        }
        .image-content {
            flex: 1;
            max-width: 100vw;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .image-content img {
            max-width: 95vw;
            max-height: 80vh;
            border-radius: 12px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.15);
            display: block;
            margin: 0 auto;
        }
        .problem-box, .solution-box {
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            color: white;
            box-shadow: 0 8px 25px rgba(0,0,0,0.2);
            width: 100%;
        }
        .problem-box {
            background: linear-gradient(135deg, #ff6b6b, #ee5a24);
        }
        .solution-box {
            background: linear-gradient(135deg, #00b894, #00cec9);
        }
        .highlight {
            background: linear-gradient(120deg, #f39c12 0%, #e74c3c 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-weight: bold;
        }
        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }
        .nav-btn {
            padding: 12px 24px;
            background: rgba(52, 73, 94, 0.9);
            color: white;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1rem;
            transition: all 0.3s ease;
        }
        .nav-btn:hover:not(:disabled) {
            background: rgba(44, 62, 80, 1);
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        }
        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .slide-indicator {
            position: fixed;
            top: 20px;
            right: 20px;
            background: rgba(255, 255, 255, 0.9);
            padding: 10px 20px;
            border-radius: 25px;
            font-weight: bold;
            color: #2c3e50;
            z-index: 1000;
        }
        .crazy-footer {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 100%;
            padding: 8px 30px;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(5px);
            border-top: 1px solid rgba(255, 255, 255, 0.2);
            display: flex;
            justify-content: space-between;
            align-items: center;
            z-index: 999;
        }
        .copyright {
            font-size: 0.9rem;
            font-weight: 500;
            color: #3a05fa;
        }
        .designer {
            font-size: 0.9rem;
            font-weight: 500;
            color: #040404;
        }
        .date-badge {
            display: inline-block;
            background-color: #764ba2;
            color: white;
            padding: 5px 15px;
            border-radius: 15px;
            font-size: 1rem;
            font-weight: bold;
            margin-top: 10px;
        }

        @media (max-width: 768px) {
            .slide h1 { font-size: 2.2rem; }
            .slide h2 { font-size: 1.8rem; }
            .slide p { font-size: 1rem; }
        }
    </style>
</head>
<body>
    <div class="presentation-container" id="presentation-container">
        <!-- Slides will be injected here -->
    </div>

    <div class="slide-indicator">
        <span id="current-slide">1</span> / <span id="total-slides">18</span>
    </div>

    <div class="navigation">
        <button class="nav-btn" id="prev-btn">Previous</button>
        <button class="nav-btn" id="next-btn">Next</button>
    </div>

    <footer class="crazy-footer">
        <div class="copyright">&copy; 2025 nDimension.ai</div>
        <div class="designer">designed and developed by CharanTheAiGuy</div>
    </footer>

    <script>
        const storyData = [
            {
                title: "Vanila neural nets",
                content: "The term 'vanilla neural net' refers to a simple, basic artificial neural network architecture. It's often used as a starting point for understanding more complex neural networks, as it lays the foundation for many advanced architectures. It consists of three layers: an input layer, an output layer, and at least one hidden layer in between. Each layer is made up of interconnected nodes, or 'neurons.'",
                image: "Assets/1_VNN.gif" // From presentation slide on Attention, shows classification
            },
            {
                title: "One issue with vanilla neural nets",
                content: "is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs. Here are a few examples of what RNNs can look like:",
                image: "Assets/2_VNNissue.jpg" // From presentation slide on Attention, shows classification
            },
            {
                title: "Fixed-Size Inputs",
                content: "The network expects inputs that always have a specific, predetermined number of elements.",
                image: "Assets/3_FSI.png" // From presentation slide on Attention, shows classification
            },
            {
                title: "Fixed-Size Outputs",
                content: "The network always produces outputs with a fixed number of elements, regardless of the input size.",
                image: "Assets/4_FSO.png" // From presentation slide on Attention, shows classification
            },

            {
                title: "The Age of Fixed Models",
                content: "Early machine learning models, including classical regression and even powerful **Convolutional Neural Networks (CNNs)**, shared a fundamental trait: they accepted a **fixed-size input** to produce a **fixed-size output**. They excelled at classification tasks—predicting if an image was a 'cat', 'dog', or 'tiger'—but the set of possible answers was always predefined and finite.",
                image: "Assets/5_classes.jpg" // From presentation slide on Attention, shows classification
            },
            {
                title: "The Problem of Variable Sequences",
                content: "The real world, however, is not fixed. Language, audio, and financial data are sequences of varying lengths. The biggest challenge became handling cases with **variable inputs and variable outputs**. How could a model translate a 10-word sentence into a 12-word sentence? This required new architectures: one-to-many, many-to-one, and many-to-many.",
                image: "Assets/6_machine-translation.jpg" // From presentation slide 6
            },
            {
                title: "The Dawn of Recurrence: RNNs",
                content: "First proposed in the 1980s, **Recurrent Neural Networks (RNNs)** were the answer. They introduced a **loop**, allowing information to persist. An RNN processes a sequence one element at a time, using its own output from the previous step as an input for the current one. This makes an RNN inherently **autoregressive**—each new prediction is dependent on past predictions.",
                image: "Assets/7_RNN.png" // From presentation slide 9
            },
            {
                title: "This ability to process sequences makes RNNs very useful.",
                content: "Machine Translation (e.g. Google Translate) is done with 'many to many' RNNs. The original text sequence is fed into an RNN, which then produces translated text as output. Sentiment Analysis (e.g. Is this a positive or negative review?) is often done with 'many to one' RNNs. The text to be analyzed is fed into an RNN, which then produces a single output classification (e.g. This is a positive review).",
                image: "Assets/8_sentiment.jpg" // From presentation slide 9
            },            
            {
                title: "The Failing Memory of RNNs",
                content: "While RNNs could handle sequences, they suffered from the **vanishing gradient problem**. Information from early in a long sequence would degrade as it passed through the recurrent loops, making it impossible for the model to remember long-term context. The 'memory' of an RNN was frustratingly short.",
                image: "Assets/9_VG.png" // From presentation slide 36
            },
            {
                title: "When we have Vanila RNN, why do we require more architectures?",
                content: "",
                image: "Assets/10_WHYMORE.png" // From presentation slide 36
            },
            {
                title: "A Solution for Memory: LSTM",
                content: "In 1997, Sepp Hochreiter and Jürgen Schmidhuber introduced the **Long Short-Term Memory (LSTM)** network. It was an RNN with a sophisticated system of 'gates' (input, forget, and output) that allowed the network to learn what information to store, update, or delete from its memory. This largely solved the vanishing gradient problem and enabled learning of long-range dependencies.",
                image: "Assets/11_LSTM.png", // From presentation slide 48
                date: "1997"
            },
            {
                title: "A Simpler Gate: The GRU",
                content: "In 2014, researchers including Kyunghyun Cho and Yoshua Bengio introduced the **Gated Recurrent Unit (GRU)**. It simplified the LSTM architecture by combining the forget and input gates into a single 'update gate'. GRUs often perform comparably to LSTMs but are more computationally efficient.",
                image: "Assets/12_GRU.png", // From presentation slide 50
                date: "2014"
            },
            {
                title: "Sequence Learning with Multiple RNN Layers",
                content: "Sequence-to-Sequence (Seq2Seq) model Developed by Google in 2018 for use in machine translation. Seq2seq turns one sequence into another sequence. It does so by use of a recurrent neural network (RNN) or more often LSTM or GRU to avoid the problem of vanishing gradient. ",
                image: "Assets/13_S2SRNN.png", // From presentation slide 50
                date: "2018"
            },
            {
                title: "Sequence to sequence chat model",
                content: "Seq2seq chat model is a type of neural network architecture that is designed to handle tasks where the input and output are both sequences, such as in natural language processing (NLP) tasks like machine translation or chatbots. It consists of an encoder and a decoder, where the encoder processes the input sequence and generates a context vector, which is then used by the decoder to produce the output sequence.",
                image: "Assets/14_S2SCHAT.png", // From presentation slide 50
            },
            {
                title: "Neural machine translation",
                content: "Neural machine translation (NMT) is a type of machine translation that uses neural networks to translate text from one language to another. It is a sequence-to-sequence model that consists of an encoder and a decoder, where the encoder processes the input sequence and generates a context vector, which is then used by the decoder to produce the output sequence.",  
                image: "Assets/14_S2SMACHINE.png", // From presentation slide 50
            },
            {
                title: "The Context Bottleneck",
                content: "Even with LSTM/GRU, a new problem became clear. The entire context of the input sequence had to be compressed into a single, fixed-size 'context vector' (the final hidden state). This information bottleneck made it difficult for the model to handle very long sentences and nuanced context, as details would inevitably be lost in compression.",
                image: "Assets/15_FVR.jpg"
            },
            {
                title: "I want to make my network to focus more on important words , How can I do that?",
                content: "The solution to the context bottleneck was to allow the model to focus on different parts of the input sequence dynamically. Instead of compressing everything into a single vector, we could have the model 'attend' to different parts of the input at each step of output generation. This is where the **Attention Mechanism** comes in.",
                image: "Assets/16_ATTENTION.png"
            },
            {
                title: "The Breakthrough: Attention",
                content: "In 2014, the **Attention Mechanism** was introduced. Instead of relying on a single context vector, Attention allows the model to 'look back' at the hidden states of every input step. It learns to assign a weight to each input word, 'paying attention' only to the most relevant parts of the input when generating an output.",
                image: "Assets/17_ATTENTIONmech.png", // From presentation slide 89
                date: "2014"
            },
            {
                title: "Self-Attention & Multi-Head",
                content: "Attention was taken a step further with **Self-Attention**, where the model weighs the importance of all other words *within the same sequence* to better encode each word's contextual meaning. **Multi-Head Attention** allows this process to be done in parallel multiple times, letting the model focus on different types of relationships.",
                image: "Assets/18_self_multi.png"
            },
            {
                title: "What about context-awareness?",
                content: "",
                image: "Assets/19_context.png"
            },
            {
                title: "The Revolution: The Transformer",
                content: "In 2017, Google researchers published 'Attention Is All You Need', introducing the **Transformer**. It completely did away with recurrence and relied solely on Multi-Head Self-Attention. Because it wasn't sequential, it could be massively parallelized, drastically reducing training times and enabling the creation of enormous models.",
                image: "Assets/20_transformers.png", // From presentation slide 122
                date: "2017"
            },       
            {
                title: "The Rise of Autoregressive LLMs",
                content: "The Transformer architecture, particularly its decoder, is still **autoregressive**. It forms the basis for most modern Large Language Models (LLMs) like OpenAI's GPT series. These models generate incredible text by predicting the next word based on all the previous words, one token at a time.",
                image: "Assets/21_first-order-autoregressive-model-l.jpg"
            },
            {
                title: "Auto regressive models vs Non Auto regressive models",
                content: "Autoregressive models generate each element of a sequence based on previous elements. This dependency on prior elements makes Autoregressive (AR) models inherently sequential, meaning each step must be completed before the next begins. Non-autoregressive (NAR) models, on the other hand, generate all elements of a sequence simultaneously, without relying on previous elements. This allows for parallel processing, significantly speeding up generation times.",
                image: "Assets/22_ARvsNAR.jpg"
            },
            {
                title: "The New Problem: High Latency",
                content: "The biggest drawback of this autoregressive approach is **latency**. Even with powerful GPUs, because generation is sequential (one word after another), it can be slow for real-time applications. You must wait for the first word to be generated before you can start generating the second. This is the new bottleneck.",
                image: "Assets/24_Architecture-a-proposed-TTS-with-an-autoregressive-transformer-based-encoder-decoder.png"
            },
            {
                title: "A New Path: Diffusion Models",
                content: "To solve latency, research has turned to **non-autoregressive** methods. One of the most promising is the **Diffusion Model**. Inspired by thermodynamics, a diffusion model starts with pure noise and learns to gradually 'denoise' it over a series of steps into a coherent output. Crucially, these steps can be performed in a more parallel fashion.",
                image: "Assets/25_diffusion_model.png"
            },
            {
                title: "Gemini and Diffusion Models",
                content: "The term 'Gemini Diffusion' can be confusing. **Gemini** itself refers to a family of powerful, natively multimodal models from Google, built on an advanced Transformer architecture. Separately, **Diffusion models** are a class of non-autoregressive models. While Google researches both, a specific 'Gemini Diffusion' model isn't the primary public branding; they are distinct but related frontiers in AI research.",
                image: "Assets/26_GeminiDiffusion.jpg"
            },
            {
                title: "Diffusion vs. Traditional LLMs",
                content: "The key difference is the generation process. An **Autoregressive LLM** builds its output sequentially, token by token. A **Diffusion Model** refines its entire output simultaneously over a fixed number of steps. It's the difference between writing a sentence one word at a time versus sketching an entire picture and gradually adding detail everywhere at once.",
                image: "Assets/27_diffusionvsllm.png"
            },
            {
                title: "Benefits of Diffusion Models",
                content: "The primary benefit is **reduced latency during inference**. Because they are not strictly sequential, they can potentially generate entire sequences much faster than autoregressive models, making them ideal for real-time applications. They have already shown state-of-the-art results in image generation and are an active area of research for text.",
                image: "Assets/28_geminidiff.png"
            },
            {
                title: "The Future: Parallel & Multimodal",
                content: "The future of AI involves a synthesis of these ideas. We are moving towards models that are not only deeply multimodal like Gemini, but also leverage non-autoregressive techniques like diffusion to be faster and more efficient than ever before. This combination will unlock new applications and bring us closer to more general and helpful AI.",
                image: "Assets/29_model_architecture.png"
            },
            {
                title: "Closing Remarks",
                content: "The journey of AI architectures is a story of identifying limitations and inventing clever solutions. From the memory of LSTMs, to the focus of Attention, the parallelism of Transformers, and the speed of Diffusion, each step has built upon the last, taking us into an exciting new era of artificial intelligence.",
                image: "Assets/30_connect.jpg"
            }
        ];

        const container = document.getElementById('presentation-container');
        let currentSlideIdx = 0;

        function renderSlides() {
            container.innerHTML = '';
            storyData.forEach((slideData, index) => {
                const slideEl = document.createElement('div');
                slideEl.className = 'slide';
                if (index === 0) slideEl.classList.add('active');

                let boxClass = '';
                if ([1, 3, 6, 11].includes(index)) {
                    boxClass = 'problem-box';
                } else if ([2, 4, 5, 7, 9, 12].includes(index)) {
                    boxClass = 'solution-box';
                }

                slideEl.innerHTML = `
                    <div class="content-wrapper">
                        <div class="text-content">
                            <h2>${slideData.title}</h2>
                            <div class="${boxClass}">
                                <p>${slideData.content.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')}</p>
                            </div>
                            ${slideData.date ? `<div class="date-badge">Innovation: ${slideData.date}</div>` : ''}
                        </div>
                        <div class="image-content">
                            <img src="${slideData.image}" alt="${slideData.title}" onerror="this.onerror=null;this.src='https://placehold.co/600x350/7f1d1d/fee2e2?text=Image+Missing';">
                        </div>
                    </div>
                `;
                container.appendChild(slideEl);
            });
        }
        
        const slides = () => document.querySelectorAll('.slide');
        const totalSlides = storyData.length;

        document.getElementById('total-slides').textContent = totalSlides;

        function showSlide(n) {
            slides().forEach((slide, index) => {
                slide.classList.remove('active');
            });
            slides()[n].classList.add('active');
            document.getElementById('current-slide').textContent = n + 1;
            document.getElementById('prev-btn').disabled = n === 0;
            document.getElementById('next-btn').disabled = n === totalSlides - 1;
        }

        function nextSlide() {
            if (currentSlideIdx < totalSlides - 1) {
                currentSlideIdx++;
                showSlide(currentSlideIdx);
            }
        }

        function previousSlide() {
            if (currentSlideIdx > 0) {
                currentSlideIdx--;
                showSlide(currentSlideIdx);
            }
        }

        document.getElementById('prev-btn').addEventListener('click', previousSlide);
        document.getElementById('next-btn').addEventListener('click', nextSlide);

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            }
        });

        // Initial render
        renderSlides();
        showSlide(0);

    </script>
</body>
</html>
